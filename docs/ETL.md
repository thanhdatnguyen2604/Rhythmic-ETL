# Rhythmic ETL Guide

This document describes in detail the ETL (Extract, Transform, Load) process in the Rhythmic-ETL project, from data collection to processing and storage.

## System Architecture Overview

The Rhythmic ETL system consists of the following main components:

1. **Kafka**: Receives streaming data from Eventsim and other sources
2. **Flink**: Processes streaming data in real-time
3. **GCS (Google Cloud Storage)**: Stores processed data
4. **Airflow**: Orchestrates data processing tasks on schedule

## System Installation

### Environment Setup

The project runs on 3 separate VMs on GCP:

1. **kafka-vm**: Contains Kafka and Eventsim
2. **flink-vm**: Contains Flink for streaming data processing
3. **airflow-vm**: Contains Airflow for task orchestration

### Setup VMs

On each VM, perform the following basic installation steps:

```bash
# Install Git, Docker and necessary tools
sudo apt-get update
sudo apt-get install -y git docker.io docker-compose jq netcat
sudo usermod -aG docker $USER

# Restart shell or logout/login to apply Docker permissions
newgrp docker

# Clone repository
git clone <repository_url>
cd Rhythmic-ETL
```

### 1. Setup kafka-vm

```bash
# Move to kafka directory
cd kafka

# Grant execution permissions
chmod +x prepare_data.sh

# Prepare data (download Million Song Dataset)
./prepare_data.sh

# Start services
docker-compose up -d
```

After running, check status:

```bash
# View containers
docker-compose ps

# Check logs
docker-compose logs
```

### 2. Setup flink-vm

Before running, prepare GCP credentials:

```bash
# Move to flink directory
cd flink

# Create secrets directory
mkdir -p secrets

# Copy GCP credentials
# Note: credentials need Storage Admin permission
cp /path/to/credentials.json secrets/cred.json

# Grant execution permissions
chmod +x *.sh

# Check installation
./check_setup.sh

# Start services
docker-compose up -d

# Run Flink jobs
./run_jobs.sh
```

### 3. Setup airflow-vm

```bash
# Move to airflow directory
cd airflow

# Create necessary directories
mkdir -p dags logs plugins config secrets

# Copy GCP credentials
cp /path/to/credentials.json secrets/cred.json

# Set permissions
chmod -R 777 logs plugins

# Initialize Airflow
docker-compose up airflow-init

# Start service
docker-compose up -d
```

Access Airflow UI at: http://airflow-vm:8080 (username: airflow, password: airflow)

## Data Flow

### 1. Data Sources

Data is generated by **Eventsim** - a tool that simulates events similar to data from a music streaming application. Eventsim uses the Million Song Dataset as the source of song data.

Event types include:
- **listen_events**: Music listening events
- **page_view_events**: Page view events
- **auth_events**: Authentication events

### 2. Kafka Streaming

Events from Eventsim are fed into Kafka through corresponding topics:
- `listen_events`
- `page_view_events`
- `auth_events`

### 3. Flink Processing

Flink reads data from Kafka, processes and normalizes it, then stores it in GCS with appropriate partitions:

```
gs://rhythmic-bucket/listen_events/year=2023/month=04/day=01/hour=23/
```

### 4. Airflow Orchestration

Airflow manages scheduled data processing tasks, including:
- **Data Validation**: Ensure new data is available in GCS
- **Batch Processing**: Perform batch analysis on data
- **Report Generation**: Create analysis reports

## Monitoring and Troubleshooting

### Check Kafka

```bash
# SSH into kafka-vm
ssh kafka-vm

# List topics
docker-compose exec kafka kafka-topics.sh --list --bootstrap-server localhost:9092

# View data in topic
docker-compose exec kafka kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic listen_events --from-beginning --max-messages 5
```

### Check Flink

```bash
# SSH into flink-vm
ssh flink-vm

# View job status
docker-compose exec jobmanager flink list

# View logs
docker-compose logs jobmanager
```

### Check Airflow

```bash
# SSH into airflow-vm
ssh airflow-vm

# View logs
docker-compose logs webserver

# View logs of specific DAG
docker-compose exec webserver airflow dags list
docker-compose exec webserver airflow tasks list <dag_id>
docker-compose exec webserver airflow dags test <dag_id> <execution_date>
```

## Common Issues and Solutions

### 1. Kafka Not Receiving Data from Eventsim

**Check**:
```bash
# Check if Eventsim is running
docker-compose ps eventsim

# Check logs
docker-compose logs eventsim
```

**Solution**:
- Ensure eventsim container is running
- Check Eventsim config.json
- Restart: `docker-compose restart eventsim`

### 2. Flink Cannot Connect to Kafka

**Check**:
```bash
# Check connection
nc -z -v kafka-vm 9092

# Check logs
docker-compose logs jobmanager
```

**Solution**:
- Ensure kafka-vm is running and accessible
- Check network configuration
- Edit environment variable: `KAFKA_BROKER=kafka-vm:9092`

### 3. Flink Cannot Save Data to GCS

**Check**:
```bash
# Check credentials
ls -la secrets/cred.json
```

**Solution**:
- Ensure credentials file exists and is properly formatted
- Check service account permissions in GCP
- Set environment variable: `export GOOGLE_APPLICATION_CREDENTIALS=/path/to/credentials.json`

### 4. Airflow DAGs Not Running

**Check**:
```bash
# Check DAG status
docker-compose exec webserver airflow dags list-runs
```

**Solution**:
- Check DAG syntax
- Ensure connections are properly configured
- Restart webserver: `docker-compose restart webserver`
